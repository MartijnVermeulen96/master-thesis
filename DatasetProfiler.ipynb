{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errorAPI\n",
    "from errorAPI.dataset import Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Type\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate new (y)?: y\n",
      "Profiling dataset beers...\n",
      "Profiling dataset company...\n",
      "Profiling dataset eeg_major...\n",
      "Profiling dataset eeg_minor...\n",
      "Profiling dataset eeg_uniform...\n",
      "Profiling dataset flights...\n",
      "Profiling dataset hospital...\n",
      "Profiling dataset kdd_major...\n"
     ]
    }
   ],
   "source": [
    "if input(\"Calculate new (y)?: \") == \"y\":\n",
    "    all_datasets = Dataset.list_datasets()\n",
    "    results = []\n",
    "\n",
    "    for d_name in all_datasets:\n",
    "        try:\n",
    "            data_dictionary = {\n",
    "                \"name\": d_name\n",
    "            }\n",
    "            d = Dataset(data_dictionary)\n",
    "            res = errorAPI.Profiler.dataset_profiler(d)\n",
    "            res[\"name\"] = d_name\n",
    "            results.append(res)\n",
    "        except:\n",
    "            print(\"Error..\")\n",
    "    dataset_profiles = pd.DataFrame.from_dict(results)\n",
    "else:\n",
    "    with open('dataset_profiles.p', 'rb') as handle:\n",
    "        dataset_profiles = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Save the results (y)?: \") == \"y\":\n",
    "    with open('dataset_profiles.p', 'wb') as handle:\n",
    "        pickle.dump(dataset_profiles, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature normalization & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "feat_columns = [x for x in dataset_profiles.columns if \"name\" not in x]\n",
    "\n",
    "X_feat = dataset_profiles[feat_columns]\n",
    "\n",
    "X_feat_norm = normalize(X_feat)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_feat_norm)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pc 1', 'pc 2'])\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2d PCA of the dataset profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(principalDf[\"pc 1\"], principalDf[\"pc 2\"])\n",
    "\n",
    "for i, txt in enumerate(dataset_profiles[\"name\"]):\n",
    "    ax.annotate(txt, (principalDf[\"pc 1\"][i], principalDf[\"pc 2\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare some close datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Display? (y): \") == \"y\":\n",
    "    display(Dataset(\"rayyan\", False).dataframe.head())\n",
    "    display(Dataset(\"movies\", False).dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = 'postgresql://postgres:postgres@localhost:5432/error_detection'\n",
    "\n",
    "performance_results = pd.read_sql_table(\"results\", create_engine(sql_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_strategies = performance_results.groupby([\"tool_name\", \"tool_configuration\"]).ngroups\n",
    "print(\"Number of strategies:\", number_of_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_threshold = 0.05\n",
    "\n",
    "group = performance_results.groupby([\"tool_name\", \"tool_configuration\"])\n",
    "new_group =  group.filter(lambda x: x['cell_f1'].mean() < f1_threshold).groupby([\"tool_name\", \"tool_configuration\"])\n",
    "number_of_filtered_strategies = new_group.ngroups\n",
    "print(\"Number of filtered strategies:\", number_of_filtered_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max performance per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxbudget_a = widgets.FloatText(description=\"Human cost\")\n",
    "maxbudget_b = widgets.FloatSlider(description=\"Human cost\")\n",
    "maxbudget_link = widgets.jslink((maxbudget_a, 'value'), (maxbudget_b, 'value'))\n",
    "maxbudget_a.value = 50\n",
    "maxruntime = widgets.FloatText(description=\"Max runtime\")\n",
    "maxruntime.value = 100000\n",
    "\n",
    "min_prec = widgets.FloatText(description=\"Min precision\", min=0, max=1.0, step=0.05)\n",
    "min_rec = widgets.FloatText(description=\"Min recall\", min=0, max=1.0, step=0.05)\n",
    "min_f1 = widgets.FloatText(description=\"Min F1\", min=0, max=1.0, step=0.05)\n",
    "\n",
    "\n",
    "def display_result(obj):\n",
    "    clear_output(wait=True)\n",
    "    display(maxbudget_a,maxbudget_b)\n",
    "    display(maxruntime)\n",
    "    \n",
    "    display(min_prec)\n",
    "    display(min_rec)\n",
    "    display(min_f1)\n",
    "    \n",
    "    performance_results_filtered = performance_results[performance_results[\"human_cost\"].fillna(0) <= maxbudget_a.value]\n",
    "    performance_results_filtered = performance_results_filtered[performance_results_filtered[\"runtime\"] <= maxruntime.value]\n",
    "    \n",
    "    performance_results_filtered = performance_results_filtered[performance_results_filtered[\"cell_prec\"] >= min_prec.value]\n",
    "    performance_results_filtered = performance_results_filtered[performance_results_filtered[\"cell_rec\"] >= min_rec.value]\n",
    "    performance_results_filtered = performance_results_filtered[performance_results_filtered[\"cell_f1\"] >= min_f1.value]\n",
    "\n",
    "    max_idx = performance_results_filtered.groupby(['dataset'])['cell_f1'].transform(max) == performance_results_filtered['cell_f1']\n",
    "    display(performance_results_filtered[max_idx].drop_duplicates(\"dataset\"))\n",
    "    filtered_keys = performance_results_filtered.index\n",
    "\n",
    "    \n",
    "display_result(None)\n",
    "\n",
    "maxbudget_a.observe(display_result)\n",
    "maxbudget_b.observe(display_result)\n",
    "maxruntime.observe(display_result)\n",
    "\n",
    "min_prec.observe(display_result)\n",
    "min_rec.observe(display_result)\n",
    "min_f1.observe(display_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_configs = performance_results.groupby([\"tool_name\", \"tool_configuration\"]).groups.keys()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Comment following line to re-test the regressors\n",
    "results = {'LR': 82.17617899805329, 'KNR': 999999999, 'RR': 70.30382126855443, 'BRR': 59.598055857113444, 'DTR': 24.10530237808723, 'SVR': 21.521600320234903, 'GBR': 17.057838379461177, 'ABR': 22.205547043776853, 'MLR': 160.0334744185594}\n",
    "\n",
    "normalize = True\n",
    "pca = -1\n",
    "\n",
    "\n",
    "if len(results) == 0:\n",
    "\n",
    "    for regressor in errorAPI.Profiler.available_regressors:\n",
    "        profiler = errorAPI.Profiler(regressor, normalize, pca)\n",
    "        profiler.train_all_configs(all_configs, dataset_profiles, performance_results)\n",
    "        print(\"Regressor:\", regressor)\n",
    "        MSE = profiler.get_MSE()\n",
    "        print(\"MSE:\", MSE)\n",
    "        results[regressor] = MSE\n",
    "        print()\n",
    "        print(\"-=\"*10)\n",
    "        print()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor = min(results, key=results.get)\n",
    "print(\"The best regressor to estimate the performance is:\", best_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = errorAPI.Profiler(best_regressor)\n",
    "profiler.train_all_configs(all_configs, dataset_profiles, performance_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave on out results of the regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.get_top_n_real(\"beers\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.get_top_n_estimated(\"beers\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now estimating a \"new\" dataset profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_set = Dataset(\"beers\")\n",
    "profiler.new_estimated_top(new_set, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_results = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in Dataset.list_datasets():\n",
    "    print(\"-\"*5, dataset_name, \"-\"*5)\n",
    "    try:\n",
    "        estimated_performance_top = profiler.get_top_n_estimated(dataset_name, number_of_results)\n",
    "        real_performance_top = profiler.get_top_n_real(dataset_name, number_of_results)\n",
    "\n",
    "        estimated_performance_list = list(estimated_performance_top.index)\n",
    "        estimated_performance_list.reverse()\n",
    "        ranking_results = []\n",
    "\n",
    "        real_rank = 0\n",
    "        for config_key in real_performance_top.index:\n",
    "            real_rank += 1\n",
    "            if config_key in estimated_performance_list:\n",
    "                rel_i = (estimated_performance_list.index(config_key) + 1) / len(estimated_performance_list)\n",
    "            else:\n",
    "                rel_i = 0\n",
    "\n",
    "            best_rel_i = (len(real_performance_top) - real_rank + 1) / len(real_performance_top)\n",
    "            \n",
    "            score = (2**rel_i - 1) / math.log2(real_rank + 1)\n",
    "            best_score = (2**best_rel_i - 1) / math.log2(real_rank + 1)\n",
    "            \n",
    "            ranking_results.append({\"config\": config_key, \"rel_i\": rel_i, \"best_rel\": best_rel_i, \"real_rank\": real_rank, \"score\": score, \"best_score\": best_score})\n",
    "\n",
    "        ranking_df = pd.DataFrame(ranking_results)\n",
    "        dcg_rank = ranking_df[\"score\"].sum()\n",
    "        idcg_rank = ranking_df[\"best_score\"].sum()\n",
    "        ndcg_rank = dcg_rank / idcg_rank\n",
    "        print(\"DCG:\", dcg_rank)\n",
    "        print(\"nDCG:\", ndcg_rank)\n",
    "    except:\n",
    "        print(\"Not calculated\")\n",
    "        \n",
    "ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
